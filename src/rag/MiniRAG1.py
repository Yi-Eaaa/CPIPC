import os
import sys
import uuid

from typing import Literal
from pydantic import BaseModel
from typing import List, Dict

from src.llm.api import SiliconFlowAPI
from src.config.config import GLOABLE_CONFIG
from src.llm.operate import hybrid_response
from src.retriever.dense_retriever import DenseRetriever
from src.retriever.bm25_retriever import BM25Retriever
from src.rag.logger import Logger

from langgraph.graph import END, StateGraph, START
from langgraph.types import Command, interrupt, Interrupt
from langgraph.checkpoint.memory import MemorySaver

# è®¾ç½®é¡¹ç›®è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
if project_root not in sys.path:
    sys.path.append(project_root)

# åˆå§‹åŒ–æ—¥å¿—
logger = Logger(log_name="MiniRAG").get_logger()

# åˆå§‹åŒ– LLM
API_KEY = GLOABLE_CONFIG["chat_api_key"]
MODEL = GLOABLE_CONFIG["chat_model"]
llm = SiliconFlowAPI(API_KEY)

# å®šä¹‰çŠ¶æ€
class RAGState(BaseModel):
    query: str
    subquestions: List[str] = []
    answers: Dict[str, str] = {}
    current_depth: int = 1
    route_decision: str = ""
    user_decision: str = ""
    retry_times: int = 0
    human_suggestion: str = ""

# HITL èŠ‚ç‚¹
from langgraph.graph import END

# æ£€æŸ¥èŠ‚ç‚¹
def check_node(state: RAGState) -> RAGState:
    logger.info(f"ğŸ” æ£€æŸ¥é—®é¢˜ï¼š{state.query}ï¼ˆæ·±åº¦ {state.current_depth}ï¼‰")
    retriever_vector = DenseRetriever()
    retriever_bm25 = BM25Retriever()
    vector_docs = retriever_vector.retrieve(state.query, 1)
    bm25_docs = retriever_bm25.retrieve(state.query, 1)
    answer = hybrid_response(state.query, vector_docs, bm25_docs)

    logger.info(f"answer: {answer}")

    if answer == "INSUFFICIENT" or answer == "INSUFFICIENT.":
        logger.info(f"âš ï¸ æ— æ³•ç›´æ¥å›ç­”ï¼š{state.query}")
        state.answers[state.query] = ""
        if state.current_depth >= 3:
            logger.info(f"âŒ è¾¾åˆ°æœ€å¤§æ·±åº¦ï¼Œæ— æ³•å›ç­”ï¼š{state.query}")
            state.route_decision = "combine"
        else:
            logger.info(f"ğŸ”„ éœ€è¦è§„åˆ’å­é—®é¢˜ï¼š{state.query}")
            state.route_decision = "planner"
    else:
        logger.info(f"âœ… å›ç­”æˆåŠŸï¼š{state.query} => {answer[:50]}...")
        state.answers[state.query] = answer
        state.route_decision = "combine"

    return state

def check_route(state: RAGState) -> str:
    logger.info(f"ğŸš¦ è·¯ç”±å†³ç­–ï¼š{state.query} -> {state.route_decision}")
    return state.route_decision

# è§„åˆ’å­é—®é¢˜
def planner_node(state: RAGState) -> RAGState:
    logger.info(f"ğŸ§© è§„åˆ’å­é—®é¢˜ï¼š{state.query}ï¼ˆæ·±åº¦ {state.current_depth}ï¼‰")
    if state.human_suggestion:
        prompt = f"ç”¨æˆ·å»ºè®®ï¼š{state.human_suggestion}\n\nè¯·å°†è¿™ä¸ªå¤æ‚é—®é¢˜æ‹†è§£ä¸ºå¤šä¸ªç®€å•å­é—®é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªï¼š\né—®é¢˜ï¼š{state.query}"
    else:
        prompt = f"è¯·å°†ä¸‹é¢è¿™ä¸ªå¤æ‚é—®é¢˜æ‹†è§£æˆå¤šä¸ªå¯å›ç­”çš„å­é—®é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªï¼š\né—®é¢˜ï¼š{state.query}"
    response = llm.chat(MODEL, prompt)
    subqs = [q.strip() for q in response.split("\n") if q.strip()]
    state.subquestions = subqs
    logger.info(f"ğŸ“Œ ç”Ÿæˆå­é—®é¢˜ï¼š{subqs}")

    for subq in subqs:
        if subq not in state.answers:
            logger.info(f"ğŸ” é€’å½’å¤„ç†å­é—®é¢˜ï¼š{subq}")
            sub_state = RAGState(
                query=subq,
                subquestions=[],
                answers=state.answers,
                current_depth=state.current_depth + 1
            )

            config = {"configurable": {"thread_id": uuid.uuid4()}}
            result = app.invoke(sub_state, config=config)
            state_obj = RAGState(**result)
            state.answers.update(state_obj.answers)


    all_sub_answers = "\n".join(f"{k}: {v}" for k, v in state.answers.items() if k != state.query)
    prompt = f"ä»¥ä¸‹æ˜¯å¯¹æ¯ä¸ªå­é—®é¢˜çš„å›ç­”ï¼Œè¯·æ•´åˆæˆä¸€ä¸ªå®Œæ•´è¿è´¯çš„ç­”æ¡ˆæ¥å›ç­”ï¼š{state.query}\n\n{all_sub_answers}"
    combined = llm.chat(MODEL, prompt)
    logger.info(f"ğŸ§  ä¸»é—®é¢˜ç­”æ¡ˆï¼š{state.query} => {combined}\n")
    logger.info("\nğŸ“š å­é—®é¢˜åŠå›ç­”ï¼š")
    for k, v in state_obj.answers.items():
        if k != query:
            logger.info(f"- {k}: {v}")
    state.answers[state.query] = combined
    return state

# Next steps after approval
def exit_node(state: RAGState) -> RAGState:
    return state

def clear_except_retry(state: RAGState, suggestion: str) -> dict:
    return {
        "query": state.query,
        "subquestions": [],
        "answers": {},
        "current_depth": 1,
        "route_decision": "",
        "user_decision": "",
        "retry_times": state.retry_times + 1,
        "human_suggestion": suggestion
    }


def combine_node(state: RAGState) -> Command[Literal["check", "exit"]]:
    if state.current_depth == 1:
        logger.info(f"âœ… è¾¾åˆ°é¡¶å±‚é—®é¢˜ï¼Œäººå·¥ä»‹å…¥")
        user_decision = ""
        # ä»ç”¨æˆ·è¾“å…¥å†³ç­–
        while True:
            user_decision = input("ğŸ‘‰ è¯·è¾“å…¥ 'approve' æˆ– 'retry': ").strip().lower()
            if user_decision in ("approve", "retry"):
                break
            print("â— è¾“å…¥æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
        print(f"ğŸ‘‰ ç”¨æˆ·è¾“å…¥ä¸ºï¼š{user_decision}")

        if user_decision == "approve":
            logger.info(f"âœ… ç”¨æˆ·æ‰¹å‡†ï¼Œè¿”å›æœ€ç»ˆç­”æ¡ˆ")
            return Command(goto="exit", update={"decision": "approve"})
        elif user_decision == "retry":
            suggestion = input("ğŸ’¡ è¯·è¾“å…¥ä½ å¯¹å¦‚ä½•æ‹†è§£è¿™ä¸ªé—®é¢˜çš„å»ºè®®ï¼ˆå¯é€‰ï¼‰: ").strip()
            logger.info(f"ğŸ”„ ç”¨æˆ·é€‰æ‹©é‡è¯•ï¼Œå»ºè®®ä¸ºï¼š{suggestion}")
            return Command(goto="check", update=clear_except_retry(state, suggestion))
    else:
        logger.info(f"âŒ æœªè¾¾åˆ°é¡¶å±‚é—®é¢˜ï¼Œæ— éœ€äººå·¥ä»‹å…¥")
        return state

# æ„å»ºii
workflow = StateGraph(RAGState)
workflow.add_node("check", check_node)
workflow.add_node("planner", planner_node)
workflow.add_node("combine", combine_node)
workflow.add_node("exit", exit_node)

workflow.set_entry_point("check")
workflow.add_conditional_edges("check", check_route, {
    "planner": "planner",
    "combine": "combine"
})
workflow.add_edge("planner", "combine")
workflow.add_edge("exit", END)

# å¯ç”¨ Checkpointer
checkpointer = MemorySaver()
app = workflow.compile(checkpointer=checkpointer)

# å¯è§†åŒ–å›¾ç»“æ„
with open("graph_output.png", "wb") as f:
    f.write(app.get_graph().draw_mermaid_png())
logger.info("ğŸ“ˆ æµç¨‹å›¾å·²ä¿å­˜ä¸º 'graph_output.png'")

if __name__ == "__main__":
    query = "Who are the protagonists in *A Christmas Carol* and *What I Worked On* respectively?"
    logger.info(f"\nğŸš€ å¯åŠ¨ä¸»æµç¨‹ï¼Œé—®é¢˜ä¸ºï¼š{query}")
    init_state = RAGState(query=query)
    config = {"configurable": {"thread_id": uuid.uuid4()}}
    
    result = app.invoke(init_state, config=config)

    # æ£€æŸ¥æ˜¯å¦ä¸­æ–­
    if isinstance(result, dict) and "__interrupt__" in result:
        interrupt_data = result.get("__interrupt__")
        print("ğŸ›‘ ä¸­æ–­ä¿¡æ¯ï¼š")
        print(interrupt_data)
        print("ğŸ§  LLM è¾“å‡ºï¼š", interrupt_data[0].value)

        # ä»ç”¨æˆ·è¾“å…¥å†³ç­–
        while True:
            user_decision = input("ğŸ‘‰ è¯·è¾“å…¥ 'approve' æˆ– 'reject': ").strip().lower()
            if user_decision in ("approve", "reject"):
                break
            print("â— è¾“å…¥æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")

        # æ¢å¤æµç¨‹
        final_result = app.invoke(
            Command(resume=user_decision),
            config=config
        )
        print("\nâœ… æœ€ç»ˆç»“æœï¼š")
        print(final_result)
    else:
        print("âœ… æ²¡æœ‰ä¸­æ–­ï¼Œç»“æœä¸ºï¼š", result)

    state_obj = RAGState(** result)

    print("\nâœ… æœ€ç»ˆå›ç­”ï¼š")
    print(state_obj.answers.get(query))

    print("\nğŸ“š å­é—®é¢˜åŠå›ç­”ï¼š")
    for k, v in state_obj.answers.items():
        if k != query:
            print(f"- {k}: {v}")
